\section{Chapter 3 : Prediction}
The prediction problem is to find the \textbf{best possible value} for $\hat{y}(t+k|t)$ given the \textbf{measured data} up to time t $\{y(1),...,y(N)\}$
\begin{figure}[H]
 \centering
  \includegraphics[width=.6\linewidth]{prediction}
\end{figure}
To obtain the \textbf{optimal} prediction :
\begin{enumerate}
\item We have to make a mathematical model for $\{y(1),..,y(N)\}$
\item Using the model compute the optimal solution
\end{enumerate}
To find the \textbf{best} mathematical model :
\begin{enumerate}
\item We select a class of models for time-series $y(t)=W(z,\theta) e(t)$ where $e(t)$ is a WN and $\theta$ a parameter vector.
\item We compute the prediction of $y(t)$ using the mathematical model : $$ \hat{y}(t+1|t;\theta) $$
\item $\hat{\theta} = argmin_{\theta}[\frac{1}{N}\sum\limits_{t=1}^{N}(y(t)-\hat{y}(t|t-1);\theta))^2]$
\item Find $y(t) = W(Z,\hat{\theta})e(t)$ which is the best model from prediction performance . Use this to compute $\hat{y}(N+K|N)$
\end{enumerate}
To create a \textbf{predictor} from an ARMA/ARMAX we need to define 2 tools:
\begin{itemize}
\item \textbf{All-pass filter}
\item \textbf{Canonical representation}
\end{itemize}

\subsection{All-Pass Filter}
An All-Pass Filter is a \textbf{first-order, linear, digital} filter with a special \textbf{constrained} structure:\\
\[
\boxed{ T(Z) = \frac{1}{a}\frac{Z+a}{Z+\frac{1}{a}} ,a \in \Re} 
\]
that depends on only one parameter and has a \textbf{pole} in $z= -\frac{1}{a}$ and zero in $z= -a$\\
Properties :\\
\begin{itemize}
\item \textbf{Magnitude}\\
$ |T(e^{jw})|^2 = |\frac{1}{a}\frac{e^{jw}+a}{e^{jw}+\frac{1}{a}}|^2 = \frac{1}{a}(\frac{e^{jw}+a}{e^{jw}+\frac{1}{a}})\cdot  \frac{1}{a}(\frac{e^{-jw}+a}{e^{-jw}+\frac{1}{a}})  = \frac{1}{a^2} \frac{1+a^2+2acosw}{1+\frac{1}{a^2}+\frac{2cosw}{a}} = 1$\\
An all-pass filter is characterized by a \textbf{frequency response} having \textbf{unitary magnitude} :
$$ \Gamma_y(w)= |T(e^{jw)}|^2 T_v(w) $$
An all-pass filter does not alter the \textbf{spectrum} of its input v(t).
This does \textbf{not} mean that $y(t) = v(t) $ but that they're \textbf{statistically equivalent} :
\[
\boxed{\Gamma_y(w) = \Gamma_v(w)}
\]
\[
\boxed{\gamma_y(w) = \gamma_v(w)}
\]
Input and output are \textbf{not} identical because although there is no change in amplitude ,an all-pass filter makes \textbf{a distortion in phase}.
\begin{figure}[H]
 \centering
  \includegraphics[width=.6\linewidth]{allpass}
\end{figure}
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{armaxallpass}
\end{figure}
The two representations of the ARMAX process are \textbf{equivalent}. The phase distortion added to signal e(t) is \textbf{not relevant}.
On the other hand , adding a T(Z) all-pass filter between $u(t)$ and $y(t)$ alters the behaviour of the system \textbf{critically}!!
\end{itemize}
\subsection{Canonical Representation}
An ARMA process can have $\infty$ \textbf{equivalent} representations ( there is no way to represent it in a unique way).\\
There is a special representation called \textbf{Canonical Representation}: \\
Given a SSP y(t) that can be modellded as an ARMA process: $$ y(t) = \frac{C(Z)}{A(Z)}e(t)$$ , $$ \frac{C(Z)}{A(Z)}$$ is the \textbf{canonical representation} if:
\begin{enumerate}
\item C(Z) and A(Z) have \textbf{same degree} ( relative degree is 0)
\item C(Z) and A(Z) are \textbf{coprime} ( no common factors)
\item C(Z) and A(Z) are  \textbf{monic} ( coefficient of max degree of both C(Z) and A(Z) is 1)
\item All roots of C(Z) and A(Z) are \textbf{strictly inside} the unit circle
\end{enumerate}
Does the canonical representation \textbf{always} exist?\\
\begin{description}
\item[Example]\hfill\\
$ y(t) = \frac{1+3Z^{-1}}{2+Z^{-1}}e(t-2) , e(t) \sim WN(0,1)$
\begin{itemize}
\item \textbf{Type and order}\\
ARMA type process of order 1,3 = ARMA(1,3)
\item \textbf{Canonical form}\\
$$ y(t) = \frac{Z^{-2} + 3Z^{-3}}{2+Z^{-1}} e(t)$$
1. Degree of C(Z) is 2 , degree of A(Z) is 0 $\to$ X\\
2. No common factors $\to$ OK\\
3. C(Z) is monic , A(Z) is not monic $\to$ X\\
4. Zero in -3 $\to$ X \\
By collecting  and using an \textbf{All-Pass Filter}:
$$ y(t) =  \frac{Z^{-2}(1+3Z^{-1})}{2(1+\frac{1}{2}Z^{-1})} \cdot 3 \frac{1+\frac{1}{3}Z^{-1}}{1+3Z^{-1}}e(t)$$ 
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{canonical}
\end{figure}
Defining $\theta = \frac{3}{2}e(t-2) , \theta \sim ? $\\
The variance of $\theta = E[\theta(t)^2]= E[(\frac{3}{2}e(t-2))^2] = \frac{9}{4}\cdot 1$\\
$\theta \sim WN(0,\frac{9}{4}) ? \to$ $ \Gamma_\theta(w)= |\frac{3}{2}e^{-2jw}|^2 \cdot 1 = \frac{9}{4} $ which is constant value so $\theta \sim WN(0,\frac{9}{4})$
Finally we obtain : 
\[
\boxed{y(t) =\frac{1+\frac{1}{3}Z^{-1}}{1+\frac{1}{2}Z^{-1}}\theta(t) , \theta \sim WN(0,\frac{9}{4})}
\]
Which is an \textbf{ARMA(1,1)} $\to$ the \textbf{canonical representation} is the representation with \textbf{minimum order!}

\end{itemize}
\end{description}
\subsection{Predictor}
The predictor at time t+k , given the data up to time t is:
$$ \hat{y}(t+k|t) $$
The \textbf{prediction error} is: 
$$  \epsilon(t+k) = y(t+k)- \hat{y}(t + k|t)$$
So the \textbf{real value } is preditor + error:
$$ y(t+k|t) = \hat{y}(t+k|t) + \epsilon(t+k)$$
$$ y(t) = \hat{y}(t|t-k) + \epsilon(t) $$
The formulas as equivalent because as per hypothesis $y(t)$ is \textbf{stationary}.
\subsubsection{Optimality}
The predictor $ \hat{y}(t+k|t) $ is \textbf{optimal} if:
\begin{enumerate}
\item $ E[\hat{y}(t+k|t) \cdot \epsilon(t+k)]=0 $ , predictor and error must be \textbf{incorrelated}
\item $ E[y(t) \cdot \epsilon(t+k)] = E[y(t-1) \cdot \epsilon(t+k)]... = 0$
\end{enumerate}
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{predictoropt}
\end{figure}
The red part shows the \textbf{unpredictable} part of y(t+k) , which is the error $\epsilon(t+k)$. If error and predictor were \textbf{correlated} than some useful unused information about $\hat{y}(t+k|t)$ would be in $\epsilon(t+k)$ which means that the predictor is not optimal.\\
The same goes for $y(t) , y(t-1)...$ of point 2): the error cannot contain information about the past/present information.\\

\subsubsection{1-step ahead prediction of MA(n)}
$$ y(t) = e(t) + c_1e(t-1)+...+c_ne(t-n) , e(t) \sim WN(0,\lambda^2)$$
We assume the the MA(n) is represented in the \textbf{canonical representation} : we must make assumptions about the $4^{th}$ property.\\
Given :\\
- \textbf{Present time} : t-1 $\to c_1e(t-1)+...+c_ne(t-n)$\\
- \textbf{Future} : t $\to e(t)$\\
\begin{description}
\item[Predictor from noise]\hfill\\
The \textbf{optimal predictor} from \textbf{noise} is :
\[
\boxed{\hat{y}(t|t-1)=c_1e(t-1)+...+c_ne(t-n)}
\]
with error 
\[
\boxed{\epsilon(t) = y(t) - \hat{y}(t|t-1) = e(t)}
\]
Optimality :
\begin{itemize}
\item $E[\hat{y}(t|t-1)\epsilon(t)]= E[(c_1e(t-1)+...+c_ne(t-n))(e(t))]=0 $
\item $E[y(t-1)\epsilon(t)]=E[(e(t-1)+c_1e(t-2)+...+c_ne(t-n-1))(e(t))]=...=0$ 
\end{itemize}
Verified because of incorrelation of white noise.\\
Since WN is \textbf{unknown} and cannot be measured , a better predictor has to be chosen from \textbf{measurable data}.

\item[Predictor from data]\hfill\\
TF:
$$ y(t) = (1+c_1Z^{-1}+...+c_nZ^{-n})e(t)$$
Inverse TF (\textbf{Whitening Filter}):
$$ e(t) = \frac{1}{1+c_1Z^{-1}+...+c_nZ^{-n}}y(t)$$
$$ \hat{y}(t|t-1) = (1+c_1Z^{-1}+...+c_nZ^{-n})e(t) \to \hat{y}(t|t-1) = \frac{c_1Z^{-1}+...+c_nZ^{-n}}{1+c_1Z^{-1}+...+c_nZ^{-n}}y(t) $$
Collecting $Z^{-1}$ :
\[
\boxed{\hat{y}(t|t-1) = \frac{c_1+...+c_nZ^{-n+1}}{1+c_1Z^{-1}+...+c_nZ^{-n}}y(t-1)}
\]
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{mapredictor}
\end{figure}
As seen in time-domain representation the prediction makes use of \textbf{present and past} data as well as \textbf{past predictions}.
\end{description}

\subsubsection{K-steps ahead predictor of MA(n)}
$$ y(t) = e(t)+c_1e(t-1)+...+c_{k-1}e(t-k+1)+c_ke(t-k)+...+c_ne(t-n)$$
Given:\\
-\textbf{Present time}: k $\to c_ke(t-k)+...+c_ne(t-n)$\\
-\textbf{Future} : t $\to e(t)+...+c_{k-1}e(t-k+1)$\\
\begin{description}
\item[Predictor from noise]\hfill\\
\[
\boxed{\hat{y}(t|t-k)=c_ke(t-k)+...+c_ne(t-n)}
\]
with error:
\[
\boxed{\epsilon(t)=e(t)+...+c_{k-1}e(t-k+1)}
\]
\item[Predictor from data]\hfill\\
\[
\boxed{\hat{y}(t|t-k)=\frac{c_k+c_{k+1}Z^{-1}+...+c_nZ^{-n+k}}{1+c_1Z^{-1}+...+c_nZ^{-n}}y(t-k)}
\]
\end{description}

\subsubsection{K-steps ahead predictor of general ARMA(m,n)}
$$ y(t) = \frac{C(Z)}{A(Z)}e(t) , e(t) \sim WN(0,\lambda^2)$$
(Under the hypothesis of \textbf{canonical representation})\\
The AR(m) part presents a recursion : need to introduce k-steps \textbf{polynomial division} between C(Z) and A(Z) obtaining :\\
- \textbf{E(Z)} $\to$ result (quotient)\\
- \textbf{R(Z)} $\to$ residual (remainder) \\
\begin{figure}[H]
 \centering
  \includegraphics[width=.7\linewidth]{poldivision}
\end{figure}
$$C(Z)=E(Z)A(Z) + R(Z)$$
\[
\boxed{ \frac{C(Z)}{A(Z)}=E(Z) + \frac{R(Z)}{A(Z)}}
\]
Noting that in k-steps division R(Z) can be rewritten by collecting $Z^{-k}$: 
$$ R(Z) = Z^{-k}\tilde{R}(Z)$$
\[
\boxed{ \frac{C(Z)}{A(Z)}=E(Z) + \frac{Z^{-k}\tilde{R}(Z))}{A(Z)}}
\]
The new transfer function is: 
$$ y(t) = [E(Z) + \frac{Z^{-k}\tilde{R}(Z))}{A(Z)}] e(t) $$
$$ y(t) = E(Z)e(t) + \frac{\tilde{R}(Z))}{A(Z)} e(t-k) $$
Where $ E(Z)e(t) $ is the \textbf{unpredictable part} as it depends on 
$ e(t) ,...,e(t-k+1)$

\begin{description}
\item[Predictor from noise]\hfill\\
\[
\boxed{\hat{y}(t|t-k) = \frac{\tilde{R}(Z)}{A(Z)}e(t-k)}
\]
with error:
\[
\boxed{\epsilon(t) = E(Z)e(t)}
\]
\item[Predictor from data]\hfill\\
$$ y(t) =\frac{C(Z)}{A(Z)}e(t) \xrightarrow[\text{Whitening}]{}e(t) = \frac{A(Z)}{C(Z)}y(t)$$
$$ \hat{y}(t|t+k) = \frac{\tilde{R}(Z)Z^{-k}}{A(Z)} \cdot \frac{A(Z)}{C(Z)}y(t)$$
\[
\boxed{\hat{y}(t|t-k) = \frac{\tilde{R}(Z)}{C(Z)}y(t-k)}
\]
\item[Remark 1]\hfill\\
Both the predictor from noise and data work under the assumption of SSP . The stationary property is satisfied if both A(Z) and C(Z) have all roots (poles) strictly inside the unitary circle. But this is satisfied by the $4^{th}$ condition of the canonical representation hypothesis.
\item[Remark 2]\hfill\\
$\epsilon(t) = y(t)- \hat{y}(t|t-k)=E(Z)e(t)$ where $E(Z)$ is a SSP of type \textbf{MA(k-1)}
\item[Remark 3]\hfill\\
In the case of \textbf{K=1} the polynomial division result in :\\
-\textbf{E(Z)} = 1 as both C(Z),A(Z) are monic and have same degree\\
-\textbf{R(Z)} = C(Z)-A(Z)\\
which results in 
\[
\boxed{\hat{y}(t|t-k)=\frac{C(Z)-A(Z)}{A(Z)}e(t)}
\]
\[
\boxed{\epsilon(t)=e(t)}
\]
Instead of having term R(Z) , the formula is now C(Z)-A(Z). As $R(Z)= \tilde{R}(Z)Z^{-1}$ there is a hidden $Z^{-1}$ in C(Z)-A(Z).
\end{description}

\subsection{Examples \& Exercises}

\subsubsection{Example 1}
Given a process $$ y(t)= \frac{Z+3}{2Z+1} e(t-1) , e(t) \sim WN(0,1)$$
Since the pole of the TF is $z=-\frac{1}{2}$ inside the unitary circle ,  W(Z) is asymptotically stable $\to$ y(t) is \textbf{stationary}.\\
\begin{enumerate}

\item \textbf{Compute $\gamma_y(0)$}\\
NB.: To calculate the variance it is not important fort the system to be in canonical representation\\
$y(t) = \frac{C(Z)}{A(Z)}e(t-1) $ is \textbf{not canonical} since it has \\
- Z = -3 not inside unitary circle\\
- 2Z in the A(Z) term\\
- $Z^{-1}e(t)$\\
Using an \textbf{All-Pass Filter}: 
$$ y(t) = \frac{Z+3}{2(Z+\frac{1}{2})} Z^{-1} \cdot 3 \frac{Z+\frac{1}{3}}{Z+3}e(t)$$
$$ \eta = \frac{3}{2}Z^{-1}e(t) \sim WN(0,\frac{9}{4}$$
$$ y(t) = \frac{Z+\frac{1}{3}}{Z+\frac{1}{2}}\eta(t)$$
Passing in time domain : $$ y(t)= -\frac{1}{2}y(t-1)+\eta(t) +\frac{1}{3}\eta(t-1)$$
-$m_y = E[y(t)]= -\frac{1}{2}E[y(t-1)]+\frac{4}{3}m_e \to 0$\\
-$\gamma_y(0) = E[y(t)^2] = E[(-\frac{1}{2}y(t-1) + \eta(t) + \frac{1}{3}\eta(t-1))^2]$\\
  $\gamma_y(0) = \frac{1}{4}\gamma_y(0) + \frac{9}{4} + \frac{1}{9}\frac{9}{4} - \frac{1}{3}E[y(t-1)\eta(t-1)]$\\
  $\frac{3}{4}\gamma_y(0) =\frac{10}{4} - \frac{1}{3}E[(-\frac{1}{2}y(t-2)+\eta(t-1)+\frac{1}{3}\eta(t-2))\eta(t-1)] $\\
  $\frac{3}{4}\gamma_y(0) = \frac{10}{4} -\frac{1}{3}E[\eta(t-1)^2] \to \frac{3}{4}\gamma_y(0) = \frac{10}{4} -\frac{1}{3}\frac{9}{4}$
 \[
 \boxed{\gamma_y(0)=\frac{7}{3}}
 \]

\item \textbf{Prediction for k=1}\\
Using the canonical negative power representation $$ y(t) = \frac{1+\frac{1}{3}Z^{-1}}{1+\frac{1}{2}Z^{-1}}\eta(t)$$
Applying the k=1 prediction formula $ \hat{y}(t|t-1) = \frac{C(Z)-A(Z)}{C(Z)}y(t)$ :
$$ \hat{y}(t|t-1) = \frac{1+\frac{1}{3}Z^{-1}-1-\frac{1}{2}Z^{-1}}{1+\frac{1}{3}Z^{-1}}y(t)$$
$$ \hat{y}(t|t-1) = \frac{-\frac{1}{6}}{1+\frac{1}{3}Z^{-1}}y(t-1)$$
In time domain :
\[
\boxed{\hat{y}(t|t-1) = -\frac{1}{3}\hat{y}(t-1|t-2)-\frac{1}{6}y(t-1)}
\]
\[
\boxed{\epsilon(t) = y(t) - \hat{y}(t|t-1) = E(Z)\eta(t) = \eta(t)}
\]
\[
\boxed{var[ y(t) - \hat{y}(t|t-1)]= var[\eta(t)] = \frac{9}{4}}
\]
\newpage
\item \textbf{Prediction for k=2}
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{ex_division}
\end{figure}
$$ \hat{y}(t|t-2) = \frac{R(Z)}{C(Z)}y(t) = \frac{\tilde{R}(Z)}{C(Z)}y(t-2)$$
$$ \hat{y}(t|t-2) = \frac{\frac{1}{12}}{1+\frac{1}{3}Z^{-1}}y(t-2)$$
\[
\boxed{\hat{y}(t|t-2) = -\frac{1}{3}\hat{y}(t-1|t-3) +\frac{1}{12}y(t-2)}
\]
\[
\boxed{\epsilon(t) = y(t) - \hat{y}(t|t-2) = E(Z)\eta(t) = (1-\frac{1}{6}Z^{-1})\eta(t)}
\]
\[
\boxed{var[ y(t) - \hat{y}(t|t-2)]= var[(1-\frac{1}{6}Z^{-1})\eta(t)] = \frac{37}{16}}
\]
\newpage
\item \textbf{Properties of $var[\epsilon(t)]$ as function of $k$}
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{ex_vark}
\end{figure}
- $k=0 \to var[y(t) -\hat{y}(t|t-k)] = 0$\\
- $k=1 \to var[y(t) -\hat{y}(t|t-k)] = \lambda^2$\\
- $k \to \infty \to var[y(t) -\hat{y}(t|t-k)] = \gamma_y(0)$ because when $k \to \infty$ the prediction goes to zero!\\
- $ var[y(t) -\hat{y}(t|t-k)] $ is a \textbf{monotonic (not strictly) increasing} function

\item \textbf{Prediction goodness}\\
The \textbf{Error to signal ratio} is a useful prediction measure:
$$ ESR(k) = \frac{var[y(t) - \hat{y}(t|t-k)]}{var[y(t)]}$$
For k=1 $$ESR(1) = \frac{\frac{9}{4}}{\frac{7}{3}} = 0.97$$
Which is a very bad prediction. The most trivial prediction that can be done is 
$$ \hat{y}(t|t-k) = m_y$$  (predicting the mean) which has \textbf{ESR(k)=1}.\\
For k=1 we only have a 3\% better prediction than the trivial one.\\
The predictor for k=1 is \textbf{optimal} which means that \textbf{no better } prediction can be made : the bad prediction is an intrinsic property of the process y(t).\\
 
\begin{figure}[H]
\begin{minipage}{.5\textwidth}
 \centering
  \includegraphics[width=.6\linewidth]{ex_poles}
\end{minipage}%
	\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{ex_spectrum}
\end{minipage}%
\end{figure}
By analysing the poles and zeros, it is easy to see that they're so close together that they almost cancel each other out.\\
The \textbf{spectrum} $\Gamma_y(w)$  in green is very close to that of the \textbf{white noise}: this is the reason y(t) is hard to predict 
\end{enumerate}


\subsubsection{Example 2 - Practical}
We have measured 5 data points of a signal: \\ $ y(1)=1, y(2) =\frac{1}{2}, y(3) =-\frac{1}{2}, y(4)=0, y(5)= -\frac{1}{2}$\\
With t=5 represent the present time  , make a prediction of $ \hat{y}(6|5) $.
To solve the problem we must make a mathematical modelling assumption. Since we're still not able to do this we need some interpretations models for this signal\\
Model A $$ y(t) = \frac{1}{2}y(t-1)+\frac{1}{4}y(t-2)+e(t) , e(t) \sim WN(0,\lambda^2)$$
Model B $$ y(t) = e(t) +\frac{1}{2}e(t-1),e(t) \sim WN(0,\lambda^2)$$
To determine which model is better we compute the \textbf{optimal} model assuming the chosen model is right.
\begin{itemize}
\item \textbf{Assuming Model A right}\hfill\\
$$ y(t) = \frac{1}{1-\frac{1}{2}Z^{-1}-\frac{1}{4}Z^{-2}} e(t)$$ is an AR(2) process in canonical representation (check it always!).\\
Since we're dealing with a k=1 prediction : 
$$ \hat{y}(t|t-1) = \frac{C(Z)-A(Z)}{C(Z)}y(t)$$
$$ \hat{y}(t|t-1) = \frac{1-1+\frac{1}{2}Z^{-1}+\frac{1}{4}Z^{-2}}{1}y(t)$$
$$ \hat{y}(t|t-1) = \frac{1}{2}y(t-1) + \frac{1}{4}y(t-2)$$
Substituting the data points :
$$ \hat{y}(6|5) = \frac{1}{2}y(5) + \frac{1}{4}y(4) = -\frac{1}{4}$$
\item \textbf{Assuming Model B right}\hfill\\
$$ y(t) = (1+\frac{1}{2}Z^{-1})e(t) $$ is an MA(1) process in canonical representation.\\
Since we're dealing with a k=1 prediction : 
$$ \hat{y}(t|t-1) = \frac{C(Z)-A(Z)}{C(Z)}y(t)$$
$$ \hat{y}(t|t-1) = \frac{1+\frac{1}{2}Z^{-1}-1}{1+\frac{1}{2}Z^{-1}}y(t)$$
$$ \hat{y}(t|t-1) = -\frac{1}{2}\hat{y}(t-1|t-2) +\frac{1}{2}y(t-1)$$
Substituting the data points :
$$ \hat{y}(6|5) = -\frac{1}{2}\hat{y}(5|4) + \frac{1}{2}y(5) = -\frac{1}{2}\hat{y}(5|4) -\frac{1}{4}$$
To compute $-\frac{1}{2}\hat{y}(5|4)$ we need to go back to the \textbf{initial condition } to compute all terms up to time =5:\\
-$ \hat{y}(2|1) = -\frac{1}{2}\hat{y}(1|0) + \frac{1}{2}y(1)$ by making the assumption that $-\frac{1}{2}\hat{y}(1|0) = m_y \to \frac{1}{2}$\\
-$ \hat{y}(3|2) = -\frac{1}{2}\hat{y}(2|1) + \frac{1}{2}y(2) = 0$\\
-$ \hat{y}(4|3) = -\frac{1}{2}\hat{y}(3|2) + \frac{1}{2}y(3) = -\frac{1}{4}$\\
-$ \hat{y}(5|4) = -\frac{1}{2}\hat{y}(4|3) + \frac{1}{2}y(4) = \frac{1}{8}$\\
-$ \hat{y}(6|5) = -\frac{1}{2}\hat{y}(5|4) + \frac{1}{2}y(5) = -\frac{5}{16}$\\
Our final prediction for model B is $\hat{y}(6|5) = -\frac{5}{16}$ which depends on the initial condition made assuming that $-\frac{1}{2}\hat{y}(1|0) = m_y $. It the choice of the initial condition important?
\textbf{If the system is asymptotically stable, and N is big the initial condition is not important as it will vanish.}


\end{itemize}

