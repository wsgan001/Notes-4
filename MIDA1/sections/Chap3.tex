\section{Chapter 3 : Prediction}
The prediction problem is to find the \textbf{best possible value} for $\hat{y}(t+k|t)$ given the \textbf{measured data} up to time t $\{y(1),...,y(N)\}$
\begin{figure}[H]
 \centering
  \includegraphics[width=.6\linewidth]{prediction}
\end{figure}
To obtain the \textbf{optimal} prediction :
\begin{enumerate}
\item We have to make a mathematical model for $\{y(1),..,y(N)\}$
\item Using the model compute the optimal solution
\end{enumerate}
To find the \textbf{best} mathematical model :
\begin{enumerate}
\item We select a class of models for time-series $y(t)=W(z,\theta) e(t)$ where $e(t)$ is a WN and $\theta$ a parameter vector.
\item We compute the prediction of $y(t)$ using the mathematical model : $$ \hat{y}(t+1|t;\theta) $$
\item $\hat{\theta} = argmin_{\theta}[\frac{1}{N}\sum\limits_{t=1}^{N}(y(t)-\hat{y}(t|t-1);\theta))^2]$
\item Find $y(t) = W(Z,\hat{\theta})e(t)$ which is the best model from prediction performance . Use this to compute $\hat{y}(N+K|N)$
\end{enumerate}
To create a \textbf{predictor} from an ARMA/ARMAX we need to define 2 tools:
\begin{itemize}
\item \textbf{All-pass filter}
\item \textbf{Canonical representation}
\end{itemize}

\subsection{All-Pass Filter}
An All-Pass Filter is a \textbf{first-order, linear, digital} filter with a special \textbf{constrained} structure:\\
\[
\boxed{ T(Z) = \frac{1}{a}\frac{Z+a}{Z+\frac{1}{a}} ,a \in \Re} 
\]
that depends on only one parameter and has a \textbf{pole} in $z= -\frac{1}{a}$ and zero in $z= -a$\\
Properties :\\
\begin{itemize}
\item \textbf{Magnitude}\\
$ |T(e^{jw})|^2 = |\frac{1}{a}\frac{e^{jw}+a}{e^{jw}+\frac{1}{a}}|^2 = \frac{1}{a}(\frac{e^{jw}+a}{e^{jw}+\frac{1}{a}})\cdot  \frac{1}{a}(\frac{e^{-jw}+a}{e^{-jw}+\frac{1}{a}})  = \frac{1}{a^2} \frac{1+a^2+2acosw}{1+\frac{1}{a^2}+\frac{2cosw}{a}} = 1$\\
An all-pass filter is characterized by a \textbf{frequency response} having \textbf{unitary magnitude} :
$$ \Gamma_y(w)= |T(e^{jw)}|^2 T_v(w) $$
An all-pass filter does not alter the \textbf{spectrum} of its input v(t).
This does \textbf{not} mean that $y(t) = v(t) $ but that they're \textbf{statistically equivalent} :
\[
\boxed{\Gamma_y(w) = \Gamma_v(w)}
\]
\[
\boxed{\gamma_y(w) = \gamma_v(w)}
\]
Input and output are \textbf{not} identical because although there is no change in amplitude ,an all-pass filter makes \textbf{a distortion in phase}.
\begin{figure}[H]
 \centering
  \includegraphics[width=.6\linewidth]{allpass}
\end{figure}
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{armaxallpass}
\end{figure}
The two representations of the ARMAX process are \textbf{equivalent}. The phase distortion added to signal e(t) is \textbf{not relevant}.
On the other hand , adding a T(Z) all-pass filter between $u(t)$ and $y(t)$ alters the behaviour of the system \textbf{critically}!!
\end{itemize}
\subsection{Canonical Representation}
An ARMA process can have $\infty$ \textbf{equivalent} representations ( there is no way to represent it in a unique way).\\
There is a special representation called \textbf{Canonical Representation}: \\
Given a SSP y(t) that can be modellded as an ARMA process: $$ y(t) = \frac{C(Z)}{A(Z)}e(t)$$ , $$ \frac{C(Z)}{A(Z)}$$ is the \textbf{canonical representation} if:
\begin{enumerate}
\item C(Z) and A(Z) have \textbf{same degree} ( relative degree is 0)
\item C(Z) and A(Z) are \textbf{coprime} ( no common factors)
\item C(Z) and A(Z) are  \textbf{monic} ( coefficient of max degree of both C(Z) and A(Z) is 1)
\item All roots of C(Z) and A(Z) are \textbf{strictly inside} the unit circle
\end{enumerate}
Does the canonical representation \textbf{always} exist?\\
\begin{description}
\item[Example]\hfill\\
$ y(t) = \frac{1+3Z^{-1}}{2+Z^{-1}}e(t-2) , e(t) \sim WN(0,1)$
\begin{itemize}
\item \textbf{Type and order}\\
ARMA type process of order 1,3 = ARMA(1,3)
\item \textbf{Canonical form}\\
$$ y(t) = \frac{Z^{-2} + 3Z^{-3}}{2+Z^{-1}} e(t)$$
1. Degree of C(Z) is 2 , degree of A(Z) is 0 $\to$ X\\
2. No common factors $\to$ OK\\
3. C(Z) is monic , A(Z) is not monic $\to$ X\\
4. Zero in -3 $\to$ X \\
By collecting  and using an \textbf{All-Pass Filter}:
$$ y(t) =  \frac{Z^{-2}(1+3Z^{-1})}{2(1+\frac{1}{2}Z^{-1})} \cdot 3 \frac{1+\frac{1}{3}Z^{-1}}{1+3Z^{-1}}e(t)$$ 
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{canonical}
\end{figure}
Defining $\theta = \frac{3}{2}e(t-2) , \theta \sim ? $\\
The variance of $\theta = E[\theta(t)^2]= E[(\frac{3}{2}e(t-2))^2] = \frac{9}{4}\cdot 1$\\
$\theta \sim WN(0,\frac{9}{4}) ? \to$ $ \Gamma_\theta(w)= |\frac{3}{2}e^{-2jw}|^2 \cdot 1 = \frac{9}{4} $ which is constant value so $\theta \sim WN(0,\frac{9}{4})$
Finally we obtain : 
\[
\boxed{y(t) =\frac{1+\frac{1}{3}Z^{-1}}{1+\frac{1}{2}Z^{-1}}\theta(t) , \theta \sim WN(0,\frac{9}{4})}
\]
Which is an \textbf{ARMA(1,1)} $\to$ the \textbf{canonical representation} is the representation with \textbf{minimum order!}

\end{itemize}
\end{description}
\subsection{Predictor}
The predictor at time t+k , given the data up to time t is:
$$ \hat{y}(t+k|t) $$
The \textbf{prediction error} is: 
$$  \epsilon(t+k) = y(t+k)- \hat{y}(t + k|t)$$
So the \textbf{real value } is preditor + error:
$$ y(t+k|t) = \hat{y}(t+k|t) + \epsilon(t+k)$$
$$ y(t) = \hat{y}(t|t-k) + \epsilon(t) $$
The formulas as equivalent because as per hypothesis $y(t)$ is \textbf{equivalent}.
\subsubsection{Optimality}
The predictor $ \hat{y}(t+k|t) $ is \textbf{optimal} if:
\begin{enumerate}
\item $ E[\hat{y}(t+k|t) \cdot \epsilon(t+k)]=0 $ , predictor and error must be \textbf{incorrelated}
\item $ E[y(t) \cdot \epsilon(t+k)] = E[y(t-1) \cdot \epsilon(t+k)]... = 0$
\end{enumerate}
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{predictoropt}
\end{figure}
The red part shows the \textbf{unpredictable} part of y(t+k) , which is the error $\epsilon(t+k)$. If error and predictor were \textbf{correlated} than some useful unused information about $\hat{y}(t+k|t)$ would be in $\epsilon(t+k)$ which means that the predictor is not optimal.\\
The same goes for $y(t) , y(t-1)...$ of point 2): the error cannot contain information about the past/present information.\\

\subsubsection{1-step ahead prediction of MA(n)}
$$ y(t) = e(t) + c_1e(t-1)+...+c_ne(t-n) , e(t) \sim WN(0,\lambda^2)$$
We assume the the MA(n) is represented in the \textbf{canonical representation} : we must make assumptions about the $4^{th}$ property.\\
Given :\\
- \textbf{Present time} : t-1 $\to c_1e(t-1)+...+c_ne(t-n)$\\
- \textbf{Future} : t $\to e(t)$\\
\begin{description}
\item[Predictor from noise]\hfill\\
The \textbf{optimal predictor} from \textbf{noise} is :
\[
\boxed{\hat{y}(t|t-1)=c_1e(t-1)+...+c_ne(t-n)}
\]
with error 
\[
\boxed{\epsilon(t) = y(t) - \hat{y}(t|t-1) = e(t)}
\]
Optimality :
\begin{itemize}
\item $E[\hat{y}(t|t-1)\epsilon(t)]= E[(c_1e(t-1)+...+c_ne(t-n))(e(t))]=0 $
\item $E[y(t-1)\epsilon(t)]=E[(e(t-1)+c_1e(t-2)+...+c_ne(t-n-1))(e(t))]=...=0$ 
\end{itemize}
Verified because of incorrelation of white noise.\\
Since WN is \textbf{unknown} and cannot be measured , a better predictor has to be chosen from \textbf{measurable data}.

\item[Predictor from data]\hfill\\
TF:
$$ y(t) = (1+c_1Z^{-1}+...+c_nZ^{-n})e(t)$$
Inverse TF (\textbf{Whitening Filter}):
$$ e(t) = \frac{1}{1+c_1Z^{-1}+...+c_nZ^{-n}}y(t)$$
$$ \hat{y}(t|t-1) = (1+c_1Z^{-1}+...+c_nZ^{-n})e(t) \to \hat{y}(t|t-1) = \frac{c_1Z^{-1}+...+c_nZ^{-n}}{1+c_1Z^{-1}+...+c_nZ^{-n}}y(t) $$
Collecting $Z^{-1}$ :
\[
\boxed{\hat{y}(t|t-1) = \frac{c_1+...+c_nZ^{-n+1}}{1+c_1Z^{-1}+...+c_nZ^{-n}}y(t-1)}
\]
\begin{figure}[H]
 \centering
  \includegraphics[width=.5\linewidth]{mapredictor}
\end{figure}
As seen in time-domain representation the prediction makes use of \textbf{present and past} data as well as \textbf{past predictions}.
\end{description}

\subsubsection{K-steps ahead predictor of MA(n)}
$$ y(t) = e(t)+c_1e(t-1)+...+c_{k-1}e(t-k+1)+c_ke(t-k)+...+c_ne(t-n)$$
Given:\\
-\textbf{Present time}: k $\to c_ke(t-k)+...+c_ne(t-n)$\\
-\textbf{Future} : t $\to e(t)+...+c_{k-1}e(t-k+1)$\\
\begin{description}
\item[Predictor from noise]\hfill\\
\[
\boxed{\hat{y}(t|t-k)=c_ke(t-k)+...+c_ne(t-n)}
\]
with error:
\[
\boxed{\epsilon(t)=e(t)+...+c_{k-1}e(t-k+1)}
\]
\item[Predictor from data]\hfill\\
\[
\boxed{\hat{y}(t|t-k)=\frac{c_k+c_{k+1}Z^{-1}+...+c_nZ^{-n+k}}{1+c_1Z^{-1}+...+c_nZ^{-n}}y(t-k)}
\]
\end{description}

\subsubsection{K-steps ahead predictor of general ARMA(m,n)}
$$ y(t) = \frac{C(Z)}{A(Z)}e(t) , e(t) \sim WN(0,\lambda^2)$$
(Under the hypothesis of \textbf{canonical representation})\\
The AR(m) part presents a recursion : need to introduce k-steps \textbf{polynomial division} between C(Z) and A(Z) obtaining :\\
- \textbf{E(Z)} $\to$ result (quotient)\\
- \textbf{R(Z)} $\to$ residual (remainder) \\
\begin{figure}[H]
 \centering
  \includegraphics[width=.7\linewidth]{poldivision}
\end{figure}
$$C(Z)=E(Z)A(Z) + R(Z)$$
\[
\boxed{ \frac{C(Z)}{A(Z)}=E(Z) + \frac{R(Z)}{A(Z)}}
\]
Noting that in k-steps division R(Z) can be rewritten by collecting $Z^{-k}$: 
$$ R(Z) = Z^{-k}\tilde{R}(Z)$$
\[
\boxed{ \frac{C(Z)}{A(Z)}=E(Z) + \frac{Z^{-k}\tilde{R}(Z))}{A(Z)}}
\]
The new transfer function is: 
$$ y(t) = [E(Z) + \frac{Z^{-k}\tilde{R}(Z))}{A(Z)}] e(t) $$
$$ y(t) = E(Z)e(t) + \frac{\tilde{R}(Z))}{A(Z)} e(t-k) $$
Where $ E(Z)e(t) $ is the \textbf{unpredictable part} as it depends on 
$ e(t) ,...,e(t-k+1)$

\begin{description}
\item[Predictor from noise]\hfill\\
\[
\boxed{\hat{y}(t|t-k) = \frac{\tilde{R}(Z)}{A(Z)}e(t-k)}
\]
with error:
\[
\boxed{\epsilon(t) = E(Z)e(t)}
\]
\item[Predictor from data]\hfill\\
$$ y(t) =\frac{C(Z)}{A(Z)}e(t) \xrightarrow[\text{Whitening}]{}e(t) = \frac{A(Z)}{C(Z)}y(t)$$
$$ \hat{y}(t|t+k) = \frac{\tilde{R}(Z)Z^{-k}}{A(Z)} \cdot \frac{A(Z)}{C(Z)}y(t)$$
\[
\boxed{\hat{y}(t|t-k) = \frac{\tilde{R}(Z)}{C(Z)}y(t-k)}
\]
\item[Remark 1]\hfill\\
Both the predictor from noise and data work under the assumption of SSP . The stationary property is satisfied if both A(Z) and C(Z) have all roots (poles) strictly inside the unitary circle. But this is satisfied by the $4^{th}$ condition of the canonical representation hypothesis.
\item[Remark 2]\hfill\\
$\epsilon(t) = y(t)- \hat{y}(t|t-k)=E(Z)e(t)$ where $E(Z)$ is a SSP of type \textbf{MA(k-1)}
\item[Remark 3]\hfill\\
In the case of \textbf{K=1} the polynomial division result in :\\
-\textbf{E(Z)} = 1 as both C(Z),A(Z) are monic and have same degree\\
-\textbf{R(Z)} = C(Z)-A(Z)\\
which results in 
\[
\boxed{\hat{y}(t|t-k)=\frac{C(Z)-A(Z)}{A(Z)}e(t)}
\]
\[
\boxed{\epsilon(t)=e(t)}
\]
Instead of having term R(Z) , the formula is now C(Z)-A(Z). As $R(Z)= \tilde{R}(Z)Z^{-1}$ there is a hidden $Z^{-1}$ in C(Z)-A(Z).
\end{description}



